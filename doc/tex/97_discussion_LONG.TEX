\section{Concluding remarks:}

    The power of random variation (through recombination and/or mutation) and non-random selection
 	to produce fine-tuned `hardware' has been extensively documented at various biomechanical levels. Classic examples include
    allometric scaling and anatomical adaptations (organism), width and material of blood vessels (organ), % \cite{holzapfel_biomechanics_2014},%valiant_evolvability_2009
    compartmentalization of sub-cellular components (cellular) and fast-folding and aggressively-functioning enzymes (molecular).
 	Recent reports \cite{livnat_analytical_2011, chastain_algorithms_2014} have highlighted evidence for `software' optimization in biological systems from a computational complexity perspective, proposing justification for the evolutionary advantage for the role of sex for example \cite{livnat_sex_2016}.
    %
    Such investigations into evolutionary biology through the lens of computational complexity have however hitherto been too high-abstracted. Nonetheless, the computational-complexity perspective has potential to be the much-needed theoretical framework that can guide the process of turning massive volumes of biological datasets into actionable knowledge \cite{brenner_turing_2012}.

    There is currently  a gap between the ever increasing scale and quality of molecular interaction networks (MINs) and the oretical understanding of the origin of their architectural properties. It has long been debated whether properties such as the majority-leaves minority-hubs (mLmH) is (non-)adaptive. Here we showed how computational intractability can provide sufficient and necessary conditions for the emergence of mLmH as an adaptation to circumvent computational intractability. The model predicts the mLmH based on the fact as a gene's degree increases linearly, its optimization "ambiguity" potential increases exponentially and hence the more computationally expensive is the task of adaptively re-wiring the network away from a deleterious and into an advantageous state. We conjectured based on predictability results that there is ultimately (as experimental coverage and resolution of high-throughput interaction-detecting experiments increases) a universal $e2n$ ratio of ${\sim}$2 in all MINs. In evolutionary context, the computational cost is the number of random-variation non-random selection iterations it takes before the right set of genes have been conserved, deleted or mutated such that the total number of damaging interactions network-wide is under some tolerance threshold. \textit{in-silico} network evolution simulation experiments, where a network's chance of survival is inversely proportional to the computational cost of re-wiring it, produced mLmH-possessing networks whose degree distribution closely match those of real MINs of equal size. The simulated evolution experiments produce the same results whether the algorithm starts off with networks that are empty but grow (add-node and add-edge mutations) in size over successive  mutate-and-select generations or with fixed-size networks that are only mutated (by randomly re-assigning edges) over successive generations. The NEP model therefore provides sufficient conditions for mLmH emergence.


    Given that the computational cost of network rewiring problem as we define it is universally insurmountable (assuming \myC{P}$\neq$\myC{NP}) in the general case, the model provides a necessary condition for the emergence of mLmH. A sparse MIN where a gene has at most one interacting partner produces the easiest possible instances of the network-rewiring problem: regardless of the current evolutionary pressure, any gene can either be beneficial or damaging depending on whether the interaction it is engaged in is beneficial or damaging to the network as a whole. There is therefore no ambiguity as to which genes to conserve and which to delete/mutate in such a fully sparse network. However, such networks would necessarily contain more genes, since  functions that could have otherwise been accomplished by a single hub gene (e.g. a phosphatase targeting multiple proteins) must now be handled by a large number of specialty genes. This clearly leads to an explosion of genome size if the same biological complexity were to evolve. On the other extreme, a dense network where functions are concentrated in as few  multi-tasking hub genes as possible would lead to an exponential search space. Particularly, the number of iterations of random-variation non-random selection needed before the network has been re-wired  away from a deleterious and into an advantageous state would be exponential in the number of unambiguous genes given some evolutionary pressure (which we represented in our model by the Oracle advice). That all genes in such a network are engaged in more than one interaction exponentially decreases their likelihood of being unambiguously advantageous or disadvantageous for the organism. An exponential number of iterations of random-variation and non-random selections are needed before the right set of genes have been conserved, deleted or mutated such that the total number of damaging interactions network-wide is under a tolerable threshold.

    mLmH topology is the middle ground between the two aforementioned extremes (the totally sparse but large, and highly dense but unadaptable networks): essential functions are concentrated \cite{gerstein_architecture_2012} in `hub' genes that are unlikely to be damaging in and of themselves \cite{khurana_interpretation_2013}  while continuous (and cheap) experimentation (e.g. fine-tuning micro-RNA regulation \cite{gerstein_architecture_2012}) is conducted at the periphery of the network \cite{kim_positive_2007} with loosely connected leaf genes.


    An immediate application of the model is to complement statistical tests used to infer the quality and coverage of large-scale interactome-mapping wet experiments \cite{rolland_proteome-scale_2014} or \textit{in-silico} network inference \cite{mitra_integrative_2013}, by testing whether the resulting networks over- or under-represents real interactions relative to the prediction. There are aspects of the model that should be extended. In this work we treated all interactions as equal, but in reality some interactions are more potent than others. Future work could extend the model by considering the potency of each interaction, a not so trivial task since no large-scale data exist yet to facilitate its inference. The model is also static, in the sense that assigning benefit/damage to a gene is based on immediate neighbours only. The implication is that all genes are equal, but in reality a central gene (many shortest paths pass through it) has much more effect network-wide than a gene residing at the periphery of the network. Trivially, a dynamic variant is where the cascading effect of a gene's beneficial (damaging) effect does not change the complexity class of NEP, although its simulations will be more computationally demanding.


    \begin{comment}
        % from the ENCODE paper "Architecture of the human regulatory network derived from ENCODE data":
        % Highly connected network elements (both transcription factors and targets) are under strong evolutionary
        % selection and exhibit stronger allele-specific activity (this is particularly apparent when multiple factors are involved).
        %Surprisingly, however, elements with allelic activity are under weaker selection than non-allelic ones.
        %  *** see also the child notes for this paper in Zotero

         Brenner recently commented: "Biological research is in crisis, and in Alan Turingâ€™s work there is much to guide us [...] we are drowning in a sea of data and thirsting for some theoretical framework with which to understand it" \cite{brenner_turing_2012}.


         This work approaches the problem from a computational complexity perspective, in contrast to the traditional approach which has been rooted in statistical mechanics.
    \end{comment}
\begin{comment}
    This can complement statistical tests used to infer the quality and coverage of large-scale interactome-mapping wet experiments \cite{rolland_proteome-scale_2014} or \textit{in-silico} network inference \cite{mitra_integrative_2013}, by testing whether the resulting networks  over- or under-represents real interactions relative to the prediction. But there is potential for insights into functional aspects of biological systems. For example, while the model was semantically interpreted here in the context of evolution (Table \ref{informal_table}), it can (without syntactic modification) also be interpreted in the context of regulation (i.e. "up-regulate"/"down-regulate" rather than "conserve"/"delete"). Through this interpretation, a new way of approaching cancer for example is possible: what system-wide alterations to a regulatory network of a healthy cell would result in the hardest optimization task to restore the total number of damaging regulatory interactions to a certain threshold? This can shed lights on the "intractable" regulatory perturbations that Nature's algorithm (random-variation/non-random selection) has not managed to proof against. Such approach can complement correlation-based studies \cite{colquhoun_investigation_2014} which (even when statistically sound) do not necessarily reveal underlying causations. Ongoing and future extensions of the model are discussed in SI \ref{I-sup_extensions}.
\end{comment}
\begin{comment}
    There are aspects of the model that should be extended. In this work we treated all interactions as equal, but in reality some interactions are more potent than others. Future work could extend the model by considering the potency of each interaction, a not so trivial task since no large-scale data exist yet to facilitate its inference. The model is also rather static, assigning benefit/damage of a gene based on immediate neighbours only. The implication is that all genes are equal, but in reality a central gene (many shortest paths pass through it) has much more effect network-wide than a gene residing at the periphery of the network. A dynamic variant is possible and (trivially) does not change the complexity class of NEP, although its simulations are likely to be more computationally demanding.
\end{comment}
% (KEGG datasets being potential resources where such knowledge could be mined).
%There is a hugely popular hub-centric view that
%biological networks grow by new nodes preferentially linking to existing hub nodes, increasing the laters' degree yet further.
%
%The results persist when tested against other MINs from various sources
%as well as against various alterations of the model.%\footnote{These results are available online at: \url{http://cs.mcgill.ca/~malsha17/EbCS/}}.
%
%For example, here we attributed benefit/damage scores to each gene according to
%how many beneficial (damaging) interactions it is projecting into or
%attracting from other genes (out- and in-degree respectively), but the results persist under projection- or attraction-only schemes.
%
%The results are also more striking if the Oracle tends, with more likelihood, to advice "conserve" the more highly connected a gene is.
%Such experiment is motivated by the fact that, in reality, highly connected genes tend to be highly conserved.
%
%The network evolution problem (NEP) definition is general enough to avoid symbolic bloat and artificial complexity
%(which hinders falsifiability) but  specific enough to capture the intricacies of biological systems.
